{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NLP Leaderboards #\n",
    "\n",
    "We want to take NLP papers and assign them to a leaderboard by task (T), dataset (D), and evaluation metric (M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train/test splits\n",
    "train_fn = \"../../data/exp/train.tsv\"\n",
    "test_fn = \"../../data/exp/test.tsv\"\n",
    "\n",
    "train_df = pd.read_csv(train_fn, sep='\\t', header=None, names=['file', 'title', 'abstract', 'exp_data', 'table', \n",
    "                                                               'task', 'dataset', 'metric'])\n",
    "test_df = pd.read_csv(test_fn, sep='\\t', header=None, names=['file', 'title', 'abstract', 'exp_data', 'table', \n",
    "                        'task', 'dataset', 'metric'])\n",
    "\n",
    "\n",
    "#print(train_df[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Files:  164\n",
      "Tasks:  44\n",
      "Datasets:  95\n",
      "Metrics:  54\n",
      "Test:\n",
      "Files:  169\n",
      "Tasks:  43\n",
      "Datasets:  91\n",
      "Metrics:  53\n",
      "Note, this is not splitting on '#' so there are fewer tasks, datasets, and metrics.\n"
     ]
    }
   ],
   "source": [
    "# print stats on train and test sets?\n",
    "print(\"Train:\")\n",
    "print(\"Files: \", train_df['file'].nunique())\n",
    "print(\"Tasks: \", train_df['task'].nunique())\n",
    "print(\"Datasets: \", train_df['dataset'].nunique())\n",
    "print(\"Metrics: \", train_df['metric'].nunique())\n",
    "print(\"Test:\")\n",
    "print(\"Files: \", test_df['file'].nunique())\n",
    "print(\"Tasks: \", test_df['task'].nunique())\n",
    "print(\"Datasets: \", test_df['dataset'].nunique())\n",
    "print(\"Metrics: \", test_df['metric'].nunique())\n",
    "print(\"Note, this is not splitting on '#' so there are fewer tasks, datasets, and metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine text fields into one\n",
    "train_df['all_text'] = train_df['title'].map(str) + train_df['abstract'].map(str) + train_df['exp_data'].map(str) + train_df['table'].map(str)\n",
    "test_df['all_text'] = test_df['title'].map(str) + test_df['abstract'].map(str) + test_df['exp_data'].map(str) + test_df['table'].map(str)\n",
    "# drop old text fields?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  6962\n",
      "Number of (train) samples:  164\n"
     ]
    }
   ],
   "source": [
    "# apply tfidfvectorizer to get features\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=False, max_df=0.95)\n",
    "# fit-transform on all data\n",
    "#all_data = np.concatenate([train_df['all_text'].values, test_df['all_text'].values])\n",
    "#all_data = train_df['all_text'].values\n",
    "#train_len = len(train_df['all_text'])\n",
    "#x_train = all_x[:train_len]\n",
    "#x_test = all_x[train_len:]\n",
    "\n",
    "# fit on train\n",
    "vectorizer = vectorizer.fit(train_df['all_text'].values)\n",
    "x_train = vectorizer.transform(train_df['all_text'].values)\n",
    "x_test = vectorizer.transform(test_df['all_text'].values)\n",
    "print(\"Number of features: \", x_train.shape[1])\n",
    "print(\"Number of (train) samples: \", x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get instance labels: multilabel or not?\n",
    "# task, data, and metrics label\n",
    "le = {}  # dict for label encoders\n",
    "y = {}  # dict for labels\n",
    "y_test = {}\n",
    "for label in ['task', 'dataset', 'metric']:\n",
    "    # old single label case\n",
    "#     le[label] = LabelEncoder().fit(train_df[label].tolist() + test_df[label].tolist())\n",
    "#     y[label] = le[label].transform(train_df[label].tolist())\n",
    "#     y_test[label] = le[label].transform(test_df[label].tolist())\n",
    "    # multilabel\n",
    "    le[label] = MultiLabelBinarizer().fit([\n",
    "        set(l.split('#')) for l in train_df[label].tolist()+test_df[label].tolist()])\n",
    "    y[label] = le[label].transform([set(l.split('#')) for l in train_df[label].tolist()])\n",
    "    y_test[label] = le[label].transform([set(l.split('#')) for l in test_df[label].tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acc ex', 'Accuracy', 'Accuracy on Average', 'Accuracy on Books',\n",
       "       'Accuracy on CNN', 'Accuracy on DVD', 'Accuracy on Daily Mail',\n",
       "       'Accuracy on Dev', 'Accuracy on Electronics',\n",
       "       'Accuracy on Kitchen', 'Accuracy on RACE', 'Accuracy on RACE-h',\n",
       "       'Accuracy on RACE-m', 'Accuracy on Test', 'Aspect (F1)',\n",
       "       'Avg Accuracy', 'Avg F1', 'BLEU', 'BLEU-1', 'BLEU-4',\n",
       "       'Bit per Character (BPC)', 'CR', 'EM', 'EM (Quasar-T)', 'Error',\n",
       "       'F0.5', 'F1', 'F1 (Quasar-T)', 'F1 (surface form)', 'F1 on Full',\n",
       "       'F1 on Newswire', 'F1-score', 'H@1', 'H@10', 'Joint goal Accuracy',\n",
       "       'LAS', 'Laptop (acc)', 'MAP', 'METEOR', 'MRR', 'Matched Accuracy',\n",
       "       'Micro-Precision', 'Mismatched Accuracy', 'N-gram F1',\n",
       "       'Number of params', 'P@10%', 'P@30%', 'P@5', 'POS',\n",
       "       'Pearson Correlation', 'Precision', 'Query Split',\n",
       "       'Question Split', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'R_10@1',\n",
       "       'R_2@1', 'Recall', 'Request Accuracy', 'Restaurant (acc)',\n",
       "       'Sentiment (acc)', 'Smatch', 'Temporal awareness',\n",
       "       'Test perplexity', 'UAS', 'Unigram Acc', 'Validation perplexity'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le['metric'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model for  task\n",
      "Trained model for  dataset\n",
      "Trained model for  metric\n"
     ]
    }
   ],
   "source": [
    "# cross validation to find good hyperparameters\n",
    "# random shuffling might not be ideal for splitting this data, but I don't think we have another option\n",
    "\n",
    "simple_clf = RandomForestClassifier()\n",
    "# simple_clf = LogisticRegressionCV()\n",
    "\n",
    "clf = {}  # dict for classifier (by label)\n",
    "for label in ['task', 'dataset', 'metric']:\n",
    "    clf[label] = MultiOutputClassifier(simple_clf, n_jobs=-1).fit(x_train, y[label])\n",
    "#     clf[label] = RandomForestClassifier(n_jobs=-1).fit(x_train, y[label])\n",
    "    print(\"Trained model for \", label)\n",
    "#     print(clf[label].get_params())\n",
    "#     print(clf[label].C_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07160987160987162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007579787234042554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesj/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05126828490467829\n"
     ]
    }
   ],
   "source": [
    "pred = {}\n",
    "pred_prob = {}\n",
    "for label in ['task', 'dataset', 'metric']:\n",
    "    pred[label] = clf[label].predict(x_test)\n",
    "    pred_prob[label] = clf[label].predict_proba(x_test)\n",
    "    score = f1_score(y_test[label], pred[label], average='macro')\n",
    "    print(score)\n",
    "#     conf_mat = confusion_matrix(metric_y, train_pred)\n",
    "#     np.savetxt(\"/tmp/foo.csv\", conf_mat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False False False False False False False False\n",
      "  False False False False  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False  True False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False  True False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False]]\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False  True False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False]\n",
      " [False False False False False False  True False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False  True False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False]]\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False  True False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False  True False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False  True False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False]\n",
      " [False False False False False False False False  True False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False  True False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False,  True, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_pred(pred, pred_prob):\n",
    "    update_pred = {}\n",
    "    for label in ['task', 'dataset', 'metric']:\n",
    "        mask = np.sum(pred[label], axis=1) > 0\n",
    "        pred_prob_np = None\n",
    "        for i in pred_prob[label]:  # use for-loop to make sure all have the right shape\n",
    "            if i.shape[1] == 1:\n",
    "                i = np.concatenate((i,np.zeros((i.shape[0],1))), axis=1)\n",
    "            assert i.shape[1] == 2, \"Only expecting 2 columns (at this point) but we have shape: %s\" % str(i.shape)\n",
    "            if pred_prob_np is None:\n",
    "                pred_prob_np = i\n",
    "            else:\n",
    "                pred_prob_np = np.concatenate((pred_prob_np, i))\n",
    "        pred_prob_np = pred_prob_np.reshape((len(pred_prob[label]),169,2))\n",
    "        am = np.argmax(pred_prob_np, axis=0)[:,1]  # need max of second column, prob label is true\n",
    "        oh = np.zeros(pred[label].shape)\n",
    "        oh[np.arange(pred[label].shape[0]), am] = 1\n",
    "        print(((oh+pred[label])>0)[:5])\n",
    "        update_pred[label] = (oh + pred[label])>0\n",
    "    return update_pred\n",
    "\n",
    "update_pred = update_pred(pred, pred_prob)\n",
    "update_pred['metric']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to CSV for evaluation\n",
    "# TODO vectorize or use new dataframe\n",
    "# want to write paper name, task, dataset, evaluation metric, score\n",
    "#for x in test_df['file'].tolist():\n",
    "#    print \n",
    "csv_df = test_df['file'].copy()\n",
    "for label in ['task', 'dataset', 'metric']:\n",
    "    csv_df = pd.concat((csv_df, pd.Series(le[label].inverse_transform(update_pred[label]))), axis=1)\n",
    "csv_df['score'] = '0.0'\n",
    "csv_df.columns = ['file', 'task', 'dataset', 'metric', 'score']\n",
    "csv_df.to_csv(\"/tmp/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
